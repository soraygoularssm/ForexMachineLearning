{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forex_rl_keras.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lHR2kUIQjRoD",
        "I0yvpeGSitjV"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHR2kUIQjRoD"
      },
      "source": [
        "# **Installing Deps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF_qVc2kjQTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493b6084-c960-45d8-ba20-2f071ffaa66b"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install gym\n",
        "!pip install keras\n",
        "!pip install keras-rl2\n",
        "!pip install finta \n",
        "!pip install oandapyV20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n",
            "Collecting keras-rl2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/fc/143ee05aed804b3b9052d7b17b13832bc7f3c28e7b1bc50edd09c29d8525/keras_rl2-1.0.5-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (57.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.31.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.4.8)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Collecting finta\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8b/94331e5e8f4e6ba2690658d4a65db0a254a89117756337316ce8f6b2026b/finta-1.3-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from finta) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from finta) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->finta) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->finta) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->finta) (1.15.0)\n",
            "Installing collected packages: finta\n",
            "Successfully installed finta-1.3\n",
            "Collecting oandapyV20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/df/560a9bc4171eab3c3b16603387bc0cedc5a9aa07d4f8835f30f51a1b7158/oandapyV20-0.7.0.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: oandapyV20\n",
            "  Building wheel for oandapyV20 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oandapyV20: filename=oandapyV20-0.7.0-cp37-none-any.whl size=70030 sha256=b13ad4a2689e7f86ad00671fdf9ccf43e82e61cc991e740e0dde274b2b863562\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/a0/2a/d96bbeddbdc532de319532d28787daddcf64459626eb1c05be\n",
            "Successfully built oandapyV20\n",
            "Installing collected packages: oandapyV20\n",
            "Successfully installed oandapyV20-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8A2nYwdwu6Z"
      },
      "source": [
        "#**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6uZGqYRw6xa"
      },
      "source": [
        "import pandas as pd\n",
        "from finta import TA\n",
        "\n",
        "import oandapyV20.endpoints.instruments as instruments\n",
        "import oandapyV20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wen5ZivOxaQa"
      },
      "source": [
        "accountID = \"\"\n",
        "access_token = \"\"\n",
        "\n",
        "client = oandapyV20.API(access_token=access_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjmRwcRxdjZ"
      },
      "source": [
        "def get_data(symbol, timeframe, count):\n",
        "    params = {\"count\": count, \"granularity\": timeframe}\n",
        "    r = instruments.InstrumentsCandles(instrument=symbol, params=params)\n",
        "    client.request(r)\n",
        "    candles = r.response['candles']\n",
        "\n",
        "    df = pd.json_normalize(candles)\n",
        "    df['High'] = df['mid.h'].astype('float64').dropna()\n",
        "    df['Low'] = df['mid.l'].astype('float64').dropna()\n",
        "    df['Open'] = df['mid.o'].astype('float64').dropna()\n",
        "    df['Close'] = df['mid.c'].astype('float64').dropna()\n",
        "    df['Volume'] = df['volume'].astype('float64').dropna()\n",
        "    df = df[['Open' , 'Close' , 'High' , 'Low' , 'Volume']]\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3cke1gD7wc7"
      },
      "source": [
        "def get_returns(pair , timeframe , count):\n",
        "  pdf = get_data(pair , timeframe , count)\n",
        "  returns = np.diff(np.log(pdf['Close'].to_numpy()))\n",
        "  returns = np.insert(returns,0,0)\n",
        "  return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZUzK_gexqBs"
      },
      "source": [
        "df = get_data('XAU_USD' , 'D' , 5000)\n",
        "# df = GOOG.copy()\n",
        "# df = EURUSD.copy()\n",
        "# df = hist\n",
        "\n",
        "spread = df['High'] - df['Low']\n",
        "average_spread = spread.rolling(20).mean()\n",
        "df['spread_ratio'] = spread / average_spread\n",
        "\n",
        "df['range'] = spread / df['Close'] - df['Low']\n",
        "\n",
        "average_volume = df['Volume'].rolling(20).mean()\n",
        "df['volume_ratio'] = df['Volume'] / average_volume\n",
        "\n",
        "df['return'] = df['Close'].pct_change()\n",
        "\n",
        "df['RSI'] = TA.RSI(df,20)\n",
        "df['STOCH'] = TA.STOCH(df,20)\n",
        "df['ADX'] = TA.ADX(df , 20)\n",
        "df['MACD'] = TA.MACD(df)['MACD']\n",
        "\n",
        "df.fillna(0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0S092XrxubC",
        "outputId": "129586cc-c9a6-40c3-b915-26828460de7c"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>spread_ratio</th>\n",
              "      <th>range</th>\n",
              "      <th>volume_ratio</th>\n",
              "      <th>return</th>\n",
              "      <th>RSI</th>\n",
              "      <th>STOCH</th>\n",
              "      <th>ADX</th>\n",
              "      <th>MACD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4309</th>\n",
              "      <td>1889.110</td>\n",
              "      <td>1898.445</td>\n",
              "      <td>1899.990</td>\n",
              "      <td>1869.945</td>\n",
              "      <td>55516.0</td>\n",
              "      <td>1.321136</td>\n",
              "      <td>-1869.929174</td>\n",
              "      <td>1.192460</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>62.003997</td>\n",
              "      <td>81.223520</td>\n",
              "      <td>32.941307</td>\n",
              "      <td>21.071038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4310</th>\n",
              "      <td>1898.470</td>\n",
              "      <td>1877.820</td>\n",
              "      <td>1903.120</td>\n",
              "      <td>1874.540</td>\n",
              "      <td>47970.0</td>\n",
              "      <td>1.250859</td>\n",
              "      <td>-1874.524780</td>\n",
              "      <td>1.018423</td>\n",
              "      <td>-0.010864</td>\n",
              "      <td>56.144460</td>\n",
              "      <td>47.992493</td>\n",
              "      <td>31.690343</td>\n",
              "      <td>18.529681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4311</th>\n",
              "      <td>1876.845</td>\n",
              "      <td>1866.160</td>\n",
              "      <td>1878.080</td>\n",
              "      <td>1844.630</td>\n",
              "      <td>54296.0</td>\n",
              "      <td>1.442009</td>\n",
              "      <td>-1844.612075</td>\n",
              "      <td>1.138732</td>\n",
              "      <td>-0.006209</td>\n",
              "      <td>53.155156</td>\n",
              "      <td>29.909009</td>\n",
              "      <td>30.557615</td>\n",
              "      <td>15.397283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4312</th>\n",
              "      <td>1866.160</td>\n",
              "      <td>1858.820</td>\n",
              "      <td>1869.205</td>\n",
              "      <td>1851.665</td>\n",
              "      <td>44211.0</td>\n",
              "      <td>0.746818</td>\n",
              "      <td>-1851.655564</td>\n",
              "      <td>0.925573</td>\n",
              "      <td>-0.003933</td>\n",
              "      <td>51.343715</td>\n",
              "      <td>19.712440</td>\n",
              "      <td>29.481523</td>\n",
              "      <td>12.182130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4313</th>\n",
              "      <td>1859.480</td>\n",
              "      <td>1857.865</td>\n",
              "      <td>1860.855</td>\n",
              "      <td>1853.265</td>\n",
              "      <td>5027.0</td>\n",
              "      <td>0.345471</td>\n",
              "      <td>-1853.260915</td>\n",
              "      <td>0.114658</td>\n",
              "      <td>-0.000514</td>\n",
              "      <td>51.105194</td>\n",
              "      <td>18.385775</td>\n",
              "      <td>28.459236</td>\n",
              "      <td>9.448125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Open     Close      High  ...      STOCH        ADX       MACD\n",
              "4309  1889.110  1898.445  1899.990  ...  81.223520  32.941307  21.071038\n",
              "4310  1898.470  1877.820  1903.120  ...  47.992493  31.690343  18.529681\n",
              "4311  1876.845  1866.160  1878.080  ...  29.909009  30.557615  15.397283\n",
              "4312  1866.160  1858.820  1869.205  ...  19.712440  29.481523  12.182130\n",
              "4313  1859.480  1857.865  1860.855  ...  18.385775  28.459236   9.448125\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0yvpeGSitjV"
      },
      "source": [
        "# **FOREX ENV**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPBkJqiRi41u"
      },
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete , Box\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9shhF2irj3vV"
      },
      "source": [
        "class ForexEnv(Env):\n",
        "  def __init__(self , df , capital , leverage , order_volume , frame_bound):\n",
        "    self.df = df\n",
        "    self.init_capital = capital\n",
        "    self.capital = capital\n",
        "    self.prev_capital = capital\n",
        "    self.leverage = leverage\n",
        "    self.order_volume = order_volume\n",
        "    self.frame_bound = frame_bound\n",
        "    self.returns , self.prices, self.signal_features = self.process_data()\n",
        "    self.shape = (self.signal_features.shape[1],)\n",
        " \n",
        "    # actions 0=buy 1=sell 2=hold\n",
        "    self.action_space = Discrete(2)\n",
        "    self.observation_space = Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float32)\n",
        "\n",
        "    self.open_position = False\n",
        "    self.order = None\n",
        "    self.ordered_at = None\n",
        "    self.ordered_at_index = None\n",
        "    self.closed_at = None\n",
        "    self.total_reward = 0\n",
        "    self.reward = 0\n",
        "    self.total_trades = 0\n",
        "    self.reward_index = 0\n",
        "\n",
        "    self.end_tick = len(self.prices) - 30\n",
        "    self.current_tick = None\n",
        "    self.done = None\n",
        "\n",
        "  def step(self , action):\n",
        "    self.done = False\n",
        "    self.current_tick += 1\n",
        "    obs = self.signal_features[self.current_tick]\n",
        "\n",
        "    if self.current_tick == self.end_tick:\n",
        "      self.done = True\n",
        "    elif self.capital <= 2*(self.init_capital / 3):\n",
        "      self.done = True\n",
        "    elif self.capital <  0.8 * self.prev_capital:\n",
        "      self.done = True\n",
        "    self.prev_capital = self.capital\n",
        "\n",
        "\n",
        "    if self.ordered_at_index:\n",
        "      if self.current_tick - self.ordered_at_index > 10:\n",
        "        self.close(self.prices[self.current_tick])\n",
        "\n",
        "    if not self.done:\n",
        "      if action == 1:\n",
        "        if self.order == 0:\n",
        "          self.close(self.prices[self.current_tick])\n",
        "          self.calculate_reward()\n",
        "        self.sell(self.prices[self.current_tick])\n",
        "      elif action == 0:\n",
        "        if self.order == 1:\n",
        "          self.close(self.prices[self.current_tick])\n",
        "          self.calculate_reward()\n",
        "        self.buy(self.prices[self.current_tick])\n",
        "\n",
        "    # reward = self.reward/1000\n",
        "    # reward *= self.capital\n",
        "    # self.total_reward += self.reward\n",
        "\n",
        "\n",
        "    info = {'initial capital' : self.init_capital , 'final capital': self.capital , 'total profit': (self.capital - self.init_capital) , 'total trades': self.total_trades}\n",
        "    return obs , self.reward , self.done , info\n",
        "\n",
        "  def render(self):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    print(self.capital , self.total_trades , self.total_reward , self.reward)\n",
        "    self.done = False\n",
        "    self.current_tick = 0\n",
        "    self.capital = self.init_capital\n",
        "    self.prev_capital = self.init_capital\n",
        "    self.open_position = False\n",
        "    self.order = None\n",
        "    self.ordered_at = None\n",
        "    self.ordered_at_index = None\n",
        "    self.closed_at = None\n",
        "    self.total_reward = 0\n",
        "    self.reward = 0\n",
        "    self.total_trades = 0\n",
        "    self.reward_index = 0\n",
        "\n",
        "    return self.signal_features[self.current_tick]\n",
        "\n",
        "\n",
        "  def close(self , close_price):\n",
        "      self.open_position = False\n",
        "      self.closed_at = close_price\n",
        "\n",
        "      order_amount = (self.capital * self.order_volume) / 100\n",
        "      order_amount = order_amount * self.leverage\n",
        "\n",
        "      #buy\n",
        "      if self.order == 0:\n",
        "          change = np.sum([self.returns[i] for i in range(self.ordered_at_index , self.current_tick) if self.returns[i]])\n",
        "          profit = order_amount * change\n",
        "          comission = order_amount / 10000\n",
        "          self.capital = self.capital + profit - comission\n",
        "\n",
        "      #sell\n",
        "      elif self.order == 1:\n",
        "          change = np.sum([self.returns[i] for i in range(self.ordered_at_index , self.current_tick) if self.returns[i]])\n",
        "          change = -change\n",
        "          profit = order_amount * change\n",
        "          comission = order_amount / 10000\n",
        "          self.capital = self.capital + profit - comission\n",
        "  \n",
        "  def buy(self , price):\n",
        "      if self.open_position:\n",
        "          pass\n",
        "      else:\n",
        "          self.total_trades += 1\n",
        "          self.open_position = True\n",
        "          self.order = 0\n",
        "          self.ordered_at = price\n",
        "          self.ordered_at_index = self.current_tick\n",
        "  \n",
        "  def sell(self , price):\n",
        "      if self.open_position:\n",
        "          pass\n",
        "      else:\n",
        "          self.total_trades += 1\n",
        "          self.open_position = True\n",
        "          self.order = 1\n",
        "          self.ordered_at = price\n",
        "          self.ordered_at_index = self.current_tick\n",
        "  \n",
        "  def calculate_reward(self):\n",
        "    step_reward = 0\n",
        "    if self.order == 0:\n",
        "      if self.prices[self.current_tick] > self.ordered_at:\n",
        "        step_reward = self.capital - self.init_capital\n",
        "    elif self.order == 1:\n",
        "      if self.prices[self.current_tick] < self.ordered_at:\n",
        "        step_reward = self.capital - self.init_capital\n",
        "\n",
        "    self.total_reward += step_reward\n",
        "\n",
        "    \n",
        "    # step_reward = 100*self.total_reward / self.total_trades\n",
        "    self.reward = step_reward\n",
        "\n",
        "  def process_data(self):\n",
        "    start = self.frame_bound[0]\n",
        "    end = self.frame_bound[1]\n",
        "    prices = self.df.loc[:, 'Close'].to_numpy()[start:end]\n",
        "    returns = np.diff(np.log(prices))\n",
        "    signal_features = self.df.loc[:, ['spread_ratio' , 'range' , 'volume_ratio' , 'return'  , 'RSI']].to_numpy()[start:end]\n",
        "    return returns , prices, signal_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_92ccKMx385"
      },
      "source": [
        "#**Create a Deep Learning Model with Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A9-WCs-z1Yp"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten , PReLU , Activation\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtWeBewhx73e"
      },
      "source": [
        "env = ForexEnv(df=df,frame_bound=(20,df.shape[0]) , capital=100 , leverage=50  , order_volume=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thnoB_VNdtG5"
      },
      "source": [
        "NODES = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwGfFKLgO5Po"
      },
      "source": [
        "def build_model(env):\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(1,env.observation_space.shape[0])))\n",
        "  model.add(Dense(24, activation='relu'))\n",
        "  model.add(Dense(24, activation='relu'))\n",
        "  model.add(Dense(env.action_space.n, activation='linear'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4SyK5EL0sX7"
      },
      "source": [
        "model = build_model(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH4O689CPUNb",
        "outputId": "b977f4be-143c-4079-e97d-125030b7cee2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                144       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 794\n",
            "Trainable params: 794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiua3fHB4qjd"
      },
      "source": [
        "# env = ForexEnv(df=df, frame_bound=(50,5000) , capital=100 , leverage=50  , order_volume=1)\n",
        "# obs = env.reset()\n",
        "# while True: \n",
        "#     obs = obs[np.newaxis, ...]\n",
        "    \n",
        "#     action , _ = model.predict(obs)\n",
        "#     action = action[0]\n",
        "#     # print(action)\n",
        "#     obs, rewards, done, info = env.step(action)\n",
        "#     if done:\n",
        "#         print(info)\n",
        "#         break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne8rcm3NPe-3"
      },
      "source": [
        "#**Build Agent with Keras-RL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNad5qkXPjgC"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy , EpsGreedyQPolicy , BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBe4ZvbPlPt"
      },
      "source": [
        "def build_agent(env):\n",
        "  nb_actions = env.action_space.n\n",
        "\n",
        "  policy = BoltzmannQPolicy()\n",
        "  memory = SequentialMemory(limit=50000, window_length=1)\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
        "                nb_actions=nb_actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "    # memory = SequentialMemory(limit=100000, window_length=1)\n",
        "\n",
        "    # policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "    #                               value_max=1.0, value_min=0.1, value_test=0.05, nb_steps=2000)\n",
        "    # nb_actions = env.action_space.n  # set up number of actions (outputs)\n",
        "\n",
        "    # # set up keras-rl agent\n",
        "    # dqn = DQNAgent(model=model, gamma=0.99, nb_actions=nb_actions, memory=memory,\n",
        "    #                batch_size=64, nb_steps_warmup=1000,\n",
        "    #                target_model_update=1e-2, policy=policy, delta_clip=1)\n",
        "\n",
        "  return dqn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNaZzykBPmnz",
        "outputId": "0bfa3911-3518-46ef-a389-0615c2c386d8"
      },
      "source": [
        "dqn = build_agent(env)\n",
        "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 100000 steps ...\n",
            "97.050226240494 17 0 0\n",
            "Interval 1 (0 steps performed)\n",
            "\r    1/10000 [..............................] - ETA: 13:27 - reward: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 1318/10000 [==>...........................] - ETA: 1:28 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 2644/10000 [======>.......................] - ETA: 1:13 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3964/10000 [==========>...................] - ETA: 1:00 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 5285/10000 [==============>...............] - ETA: 47s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 6605/10000 [==================>...........] - ETA: 33s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 7929/10000 [======================>.......] - ETA: 20s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 9249/10000 [==========================>...] - ETA: 7s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0000e+00\n",
            "7 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.045 - mae: 15.957 - mean_q: 4.077 - initial capital: 100.000 - final capital: 83.557 - total profit: -16.443 - total trades: 58.638\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "  576/10000 [>.............................] - ETA: 1:32 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 1896/10000 [====>.........................] - ETA: 1:20 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3215/10000 [========>.....................] - ETA: 1:06 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 4539/10000 [============>.................] - ETA: 53s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 5864/10000 [================>.............] - ETA: 40s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 7184/10000 [====================>.........] - ETA: 27s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 8507/10000 [========================>.....] - ETA: 14s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 9828/10000 [============================>.] - ETA: 1s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0000e+00\n",
            "8 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.021 - mae: 18.263 - mean_q: 1.449 - initial capital: 100.000 - final capital: 82.770 - total profit: -17.230 - total trades: 61.655\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            " 1152/10000 [==>...........................] - ETA: 1:26 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 2471/10000 [======>.......................] - ETA: 1:14 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3796/10000 [==========>...................] - ETA: 1:01 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 5117/10000 [==============>...............] - ETA: 48s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 6437/10000 [==================>...........] - ETA: 35s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 7760/10000 [======================>.......] - ETA: 22s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 9080/10000 [==========================>...] - ETA: 9s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0000e+00\n",
            "7 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.016 - mae: 17.775 - mean_q: 0.512 - initial capital: 100.000 - final capital: 83.255 - total profit: -16.745 - total trades: 59.791\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "  405/10000 [>.............................] - ETA: 1:34 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 1725/10000 [====>.........................] - ETA: 1:21 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3049/10000 [========>.....................] - ETA: 1:08 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 4369/10000 [============>.................] - ETA: 55s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 5691/10000 [================>.............] - ETA: 42s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 7013/10000 [====================>.........] - ETA: 29s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 8335/10000 [========================>.....] - ETA: 16s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 9657/10000 [===========================>..] - ETA: 3s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0000e+00\n",
            "8 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.014 - mae: 15.035 - mean_q: 0.211 - initial capital: 100.000 - final capital: 83.029 - total profit: -16.971 - total trades: 60.764\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "  981/10000 [=>............................] - ETA: 1:32 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 2299/10000 [=====>........................] - ETA: 1:18 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3621/10000 [=========>....................] - ETA: 1:04 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 4943/10000 [=============>................] - ETA: 51s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 6268/10000 [=================>............] - ETA: 37s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 7590/10000 [=====================>........] - ETA: 24s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 8911/10000 [=========================>....] - ETA: 10s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0000e+00\n",
            "7 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.012 - mae: 11.867 - mean_q: 0.097 - initial capital: 100.000 - final capital: 82.906 - total profit: -17.094 - total trades: 60.944\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "  234/10000 [..............................] - ETA: 1:36 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 1557/10000 [===>..........................] - ETA: 1:24 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 2875/10000 [=======>......................] - ETA: 1:12 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 4199/10000 [===========>..................] - ETA: 58s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 5522/10000 [===============>..............] - ETA: 45s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 6846/10000 [===================>..........] - ETA: 31s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 8166/10000 [=======================>......] - ETA: 18s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 9486/10000 [===========================>..] - ETA: 5s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0000e+00\n",
            "8 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.009 - mae: 8.116 - mean_q: 0.033 - initial capital: 100.000 - final capital: 83.270 - total profit: -16.730 - total trades: 59.874\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "  809/10000 [=>............................] - ETA: 1:39 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 2131/10000 [=====>........................] - ETA: 1:23 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 3456/10000 [=========>....................] - ETA: 1:08 - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 4774/10000 [=============>................] - ETA: 54s - reward: 0.0000e+0066.1981558028443 121 0 0\n",
            " 6119/10000 [=================>............] - ETA: 39s - reward: -0.525765.7490463560715 124 -2.7803447045044294 -2.7803447045044294\n",
            " 7438/10000 [=====================>........] - ETA: 26s - reward: -0.432566.1981558028443 121 0 0\n",
            " 8832/10000 [=========================>....] - ETA: 11s - reward: -1.437766.56506515425203 185 -563.8844220022099 0\n",
            " 9885/10000 [============================>.] - ETA: 1s - reward: -1.998266.08244370632235 133 -371.2796434696044 -26.08289649263979\n",
            "10000/10000 [==============================] - 101s 10ms/step - reward: -1.9844\n",
            "8 episodes - episode_reward: -2472.332 [-9480.747, 0.000] - loss: 0.395 - mae: 5.669 - mean_q: -0.658 - initial capital: 100.000 - final capital: 82.992 - total profit: -17.008 - total trades: 67.606\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            " 1096/10000 [==>...........................] - ETA: 1:28 - reward: -1.768966.06054553308155 135 -102.88689108829861 0\n",
            " 2158/10000 [=====>........................] - ETA: 1:17 - reward: -4.739865.97679499273808 117 -226.5731857220801 -27.731879645966472\n",
            " 3796/10000 [==========>...................] - ETA: 1:01 - reward: -5.066466.47927268804243 202 -40.43830886767962 -10.930468436882009\n",
            " 5105/10000 [==============>...............] - ETA: 48s - reward: -4.383166.43153890483916 161 -139.9749785253698 0\n",
            " 6431/10000 [==================>...........] - ETA: 35s - reward: -5.927266.48582039220545 173 -141.14319190671637 -17.261001085900972\n",
            " 8054/10000 [=======================>......] - ETA: 19s - reward: -5.014365.64763253938744 226 -80.3351041152673 0\n",
            " 9040/10000 [==========================>...] - ETA: 9s - reward: -5.615166.35019656693257 173 -495.330557017703 0\n",
            "10000/10000 [==============================] - 100s 10ms/step - reward: -4.7455\n",
            "7 episodes - episode_reward: -7260.812 [-15759.875, -2004.359] - loss: 8.125 - mae: 19.151 - mean_q: -29.680 - initial capital: 100.000 - final capital: 87.005 - total profit: -12.995 - total trades: 103.535\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            " 3304/10000 [========>.....................] - ETA: 1:06 - reward: -4.028972.44408987934268 497 352.0680625307822 -12.405108080648361\n",
            " 4810/10000 [=============>................] - ETA: 51s - reward: -4.448666.25636730432171 324 -209.09623377344943 -32.04250393285267\n",
            " 9073/10000 [==========================>...] - ETA: 9s - reward: 1.3083141.70955267023297 688 3382.94799924702 21.208545937709317\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: 0.6706\n",
            "3 episodes - episode_reward: 5087.003 [-10067.927, 33449.358] - loss: 21.691 - mae: 34.806 - mean_q: -63.392 - initial capital: 100.000 - final capital: 102.118 - total profit: 2.118 - total trades: 344.728\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            " 3340/10000 [=========>....................] - ETA: 1:06 - reward: -0.847192.10129150461096 632 -1922.8467162389356 0\n",
            " 4077/10000 [===========>..................] - ETA: 59s - reward: -1.436665.87238247416734 239 -998.036678974085 0\n",
            " 8346/10000 [========================>.....] - ETA: 16s - reward: -0.7297127.69052273551489 619 -793.999956472069 0\n",
            "10000/10000 [==============================] - 100s 10ms/step - reward: 3.7324\n",
            "done, took 999.707 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1b814a00d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "VPTKT5gNRUQ6",
        "outputId": "ab72a367-7f1f-47c4-95a6-11c6fd9c28c8"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "192.6899098954048 334 2979.1378027809565 39.49658074198763\n",
            "Episode 1: reward: 0.000, steps: 4264\n",
            "160.2086874281787 388 0 0\n",
            "Episode 2: reward: 0.000, steps: 4264\n",
            "160.2086874281787 388 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d8989210729d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccumulated_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                             \u001b[0maccumulated_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                         \u001b[0maccumulated_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy4wD6C4Gw72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "85feea06-af7a-4e41-cad9-6f4aeba39853"
      },
      "source": [
        "dqn.predict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e74eabc2b572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkaONoWPRUrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "211af77a-d671-4a7c-ac77-3d1366addcae"
      },
      "source": [
        "_ = dqn.test(env, nb_episodes=15, visualize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 15 episodes ...\n",
            "79.15717396707971 79 0 0\n",
            "Episode 1: reward: 0.000, steps: 1322\n",
            "66.1981558028443 121 0 0\n",
            "Episode 2: reward: 0.000, steps: 1322\n",
            "66.1981558028443 121 0 0\n",
            "Episode 3: reward: 0.000, steps: 1322\n",
            "66.1981558028443 121 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6cbe10b15ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4054\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4055\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4056\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4057\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgJyj_eRdAV"
      },
      "source": [
        "#**save model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LypCxVxYRexb"
      },
      "source": [
        "dqn.save_weights('DQN_FOREX.h5f', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s76Zsj6DnT1"
      },
      "source": [
        "#**Forex Env 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSIN0ptg3wCT"
      },
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete , Box\n",
        "from gym.utils import seeding\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p82WQLck31zL"
      },
      "source": [
        "# position constant\n",
        "LONG = 0\n",
        "SHORT = 1\n",
        "FLAT = 2\n",
        "\n",
        "# action constant\n",
        "BUY = 0\n",
        "SELL = 1\n",
        "HOLD = 2\n",
        "\n",
        "class ForexEnv2(Env):\n",
        "\n",
        "    def __init__(self, df , frame_bound, show_trade=True):\n",
        "        self.show_trade = show_trade\n",
        "        self.df = df\n",
        "        self.features = df.loc[:, ['spread_ratio' , 'range' , 'volume_ratio' , 'price_rate_of_change' , 'RSI']].to_numpy()[frame_bound[0]:frame_bound[1]]\n",
        "        self.frame_bound = frame_bound\n",
        "        self.actions = [\"LONG\", \"SHORT\", \"FLAT\"]\n",
        "        self.fee = 0.0005\n",
        "        self.seed()\n",
        "\n",
        "        self.prices = self.process_data()\n",
        "        self.shape = (self.features.shape[1] + 1,)\n",
        "\n",
        "        # defines action space\n",
        "        self.action_space = Discrete(len(self.actions))\n",
        "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float32)\n",
        "\n",
        "    def render(self, mode='human', verbose=False):\n",
        "        return None\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        if self.done:\n",
        "            return self.state, self.reward, self.done, {}\n",
        "        self.reward = 0\n",
        "\n",
        "        # action comes from the agent\n",
        "        # 0 buy, 1 sell, 2 hold\n",
        "        # single position can be opened per trade\n",
        "        # valid action sequence would be\n",
        "        # LONG : buy - hold - hold - sell\n",
        "        # SHORT : sell - hold - hold - buy\n",
        "        # invalid action sequence is just considered hold\n",
        "        # (e.g.) \"buy - buy\" would be considred \"buy - hold\"\n",
        "        self.action = HOLD  # hold\n",
        "        if action == BUY: # buy\n",
        "            if self.position == FLAT: # if previous position was flat\n",
        "                self.position = LONG # update position to long\n",
        "                self.action = BUY # record action as buy\n",
        "                self.entry_price = self.closingPrice # maintain entry price\n",
        "            elif self.position == SHORT: # if previous position was short\n",
        "                self.position = FLAT  # update position to flat\n",
        "                self.action = BUY # record action as buy\n",
        "                self.exit_price = self.closingPrice\n",
        "                self.reward += ((self.entry_price - self.exit_price)/self.exit_price + 1)*(1-self.fee)**2 - 1 # calculate reward\n",
        "                self.krw_balance = self.krw_balance * (1.0 + self.reward) # evaluate cumulative return in krw-won\n",
        "                self.entry_price = 0 # clear entry price\n",
        "                self.n_short += 1 # record number of short\n",
        "        elif action == SELL: # vice versa for short trade\n",
        "            if self.position == FLAT:\n",
        "                self.position = SHORT\n",
        "                self.action = 1\n",
        "                self.entry_price = self.closingPrice\n",
        "            elif self.position == LONG:\n",
        "                self.position = FLAT\n",
        "                self.action = 1\n",
        "                self.exit_price = self.closingPrice\n",
        "                self.reward += ((self.exit_price - self.entry_price)/self.entry_price + 1)*(1-self.fee)**2 - 1\n",
        "                self.krw_balance = self.krw_balance * (1.0 + self.reward)\n",
        "                self.entry_price = 0\n",
        "                self.n_long += 1\n",
        "\n",
        "        self.current_tick += 1\n",
        "        if(self.show_trade and self.current_tick%100 == 0):\n",
        "            print(\"Tick: {0}/ Portfolio (USD): {1}\".format(self.current_tick, self.portfolio))\n",
        "            print(\"Long: {0}/ Short: {1}\".format(self.n_long, self.n_short))\n",
        "        self.history.append((self.action, self.current_tick, self.closingPrice, self.krw_balance, self.reward))\n",
        "        self.updateState()\n",
        "        if (self.current_tick > (self.prices.shape[0]) -1 -1):\n",
        "            self.done = True\n",
        "            self.reward = self.get_profit() # return reward at end of the game\n",
        "        return self.state, self.reward, self.done, {'portfolio':np.array([self.krw_balance]),\n",
        "                                                    \"history\":self.history,\n",
        "                                                    \"n_trades\":{'long':self.n_long, 'short':self.n_short}}\n",
        "\n",
        "    def get_profit(self):\n",
        "        if(self.position == LONG):\n",
        "            profit = ((self.closingPrice - self.entry_price)/self.entry_price + 1)*(1-self.fee)**2 - 1\n",
        "        elif(self.position == SHORT):\n",
        "            profit = ((self.entry_price - self.closingPrice)/self.closingPrice + 1)*(1-self.fee)**2 - 1\n",
        "        else:\n",
        "            profit = 0\n",
        "        return profit\n",
        "\n",
        "    def reset(self):\n",
        "        try:\n",
        "          print(self.krw_balance , self.n_long, self.n_short)\n",
        "        except:\n",
        "          print('oops')\n",
        "\n",
        "        # self.current_tick = random.randint(0, self.df.shape[0]-1000)\n",
        "        self.current_tick = 0\n",
        "\n",
        "        # positions\n",
        "        self.n_long = 0\n",
        "        self.n_short = 0\n",
        "\n",
        "        # clear internal variables\n",
        "        self.history = [] # keep buy, sell, hold action history\n",
        "        self.krw_balance = 1000 # initial balance, u can change it to whatever u like\n",
        "        self.profit = 0\n",
        "\n",
        "        self.action = HOLD\n",
        "        self.position = FLAT\n",
        "        self.done = False\n",
        "        \n",
        "        self.updateState() # returns observed_features +  opened position(LONG/SHORT/FLAT) + profit_earned(during opened position)\n",
        "        return self.state\n",
        "\n",
        "\n",
        "    def updateState(self):\n",
        "        self.closingPrice = float(self.prices[self.current_tick])\n",
        "        profit = self.get_profit()\n",
        "        # append two\n",
        "        self.state = np.concatenate((self.features[self.current_tick], [profit]))\n",
        "        return self.state\n",
        "  \n",
        "    def process_data(self):\n",
        "      start = self.frame_bound[0]\n",
        "      end = self.frame_bound[1]\n",
        "      prices = self.df.loc[:, 'Close'].to_numpy()[start:end]\n",
        "      \n",
        "      return prices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxlrS58b-Vpo"
      },
      "source": [
        "#**RL Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMdjk40e-eMP"
      },
      "source": [
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import A2C , DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglmfYVi-epK"
      },
      "source": [
        "env = ForexEnv2(df=df,frame_bound=(20,df.shape[0]) , show_trade = False)\n",
        "env_maker = lambda: env\n",
        "env = DummyVecEnv([env_maker])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dnwoQKr-pJG",
        "outputId": "e3de30d5-da6e-4a78-ad77-8a1f554f3c75"
      },
      "source": [
        "model = A2C('MlpLstmPolicy', env, verbose=1)\n",
        "model.learn(total_timesteps=50000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "oops\n",
            "---------------------------------\n",
            "| explained_variance | 0.351    |\n",
            "| fps                | 16       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 5        |\n",
            "| value_loss         | 0.000745 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0611  |\n",
            "| fps                | 206      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.89e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.19    |\n",
            "| fps                | 169      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 0.000379 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0202  |\n",
            "| fps                | 140      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 2.36e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00427 |\n",
            "| fps                | 118      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 1.58e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.79    |\n",
            "| fps                | 103      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 6.73e-08 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.8     |\n",
            "| fps                | 90       |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 7.86e-09 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00175  |\n",
            "| fps                | 81       |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 1.03e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.681   |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 6.01e-06 |\n",
            "---------------------------------\n",
            "539.3985327278244 428 511\n",
            "---------------------------------\n",
            "| explained_variance | -0.116   |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 1.72e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0135   |\n",
            "| fps                | 77       |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 0.00184  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.21    |\n",
            "| fps                | 79       |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 2.88e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0121   |\n",
            "| fps                | 80       |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.000436 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.151   |\n",
            "| fps                | 80       |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 7.09e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.113   |\n",
            "| fps                | 78       |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.000443 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.176   |\n",
            "| fps                | 76       |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 9.91e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0383  |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.35e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.326    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 3.68e-08 |\n",
            "---------------------------------\n",
            "904.9029464135257 475 461\n",
            "---------------------------------\n",
            "| explained_variance | -0.146   |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 0.000384 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.104    |\n",
            "| fps                | 74       |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.000118 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.884    |\n",
            "| fps                | 75       |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 5.36e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -36.3    |\n",
            "| fps                | 76       |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 2.04e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.272    |\n",
            "| fps                | 75       |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 2.3e-05  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.838    |\n",
            "| fps                | 74       |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 2.11e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0984  |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 1.52e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0153  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.000531 |\n",
            "---------------------------------\n",
            "466.2926213740391 496 490\n",
            "---------------------------------\n",
            "| explained_variance | 0.151    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 0.000216 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.12     |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 1.12e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.268    |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.000332 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.102    |\n",
            "| fps                | 74       |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 8.32e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -4.54    |\n",
            "| fps                | 74       |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 8.46e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.12    |\n",
            "| fps                | 74       |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 2.41e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0198  |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.000654 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.138    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 3.02e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.753   |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 3.43e-06 |\n",
            "---------------------------------\n",
            "306.9395595348858 458 507\n",
            "---------------------------------\n",
            "| explained_variance | 0.425    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 0.000221 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.309    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 0.000351 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.174   |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.4e-05  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0689  |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.000274 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.109    |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.000221 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00799  |\n",
            "| fps                | 73       |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 3.38e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0909  |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 1e-05    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.395    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 3.9e-06  |\n",
            "---------------------------------\n",
            "264.94231873764136 472 514\n",
            "---------------------------------\n",
            "| explained_variance | 0.0396   |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 0.00111  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.027   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.000273 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.96    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 7.03e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.92    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 3.24e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.03    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 0.000221 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.161    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 6.37e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.135    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 5.36e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.179    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 4.26e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0315   |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 7.31e-05 |\n",
            "---------------------------------\n",
            "810.9886583346536 486 479\n",
            "---------------------------------\n",
            "| explained_variance | -12.8    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 1.78e-05 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -6.91e+03 |\n",
            "| fps                | 71        |\n",
            "| nupdates           | 5300      |\n",
            "| policy_entropy     | 1.09      |\n",
            "| total_timesteps    | 26500     |\n",
            "| value_loss         | 0.00179   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.315    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 9.52e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0597  |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 4.17e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.852   |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 0.000203 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.707   |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 1.04e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00981 |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.00103  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0315   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 6.74e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -5.98    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.000106 |\n",
            "---------------------------------\n",
            "410.26016926827924 487 446\n",
            "---------------------------------\n",
            "| explained_variance | 0.0447   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 6100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 30500    |\n",
            "| value_loss         | 0.000213 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0448   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 6200     |\n",
            "| policy_entropy     | 1.09     |\n",
            "| total_timesteps    | 31000    |\n",
            "| value_loss         | 2.72e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.642    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 6300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 31500    |\n",
            "| value_loss         | 2.91e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.122   |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 6400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 32000    |\n",
            "| value_loss         | 5.62e-07 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0419  |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 6500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 32500    |\n",
            "| value_loss         | 0.000509 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.105   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 6600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 33000    |\n",
            "| value_loss         | 7.61e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0472  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 6700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 33500    |\n",
            "| value_loss         | 5.44e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.179    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 6800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 34000    |\n",
            "| value_loss         | 0.00138  |\n",
            "---------------------------------\n",
            "553.4450966800975 494 454\n",
            "---------------------------------\n",
            "| explained_variance | 0.152    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 6900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 34500    |\n",
            "| value_loss         | 2.54e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.125    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 35000    |\n",
            "| value_loss         | 5.98e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.354    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 35500    |\n",
            "| value_loss         | 2.51e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0498   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 36000    |\n",
            "| value_loss         | 0.00264  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.225    |\n",
            "| fps                | 72       |\n",
            "| nupdates           | 7300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 36500    |\n",
            "| value_loss         | 5.2e-06  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.794   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 37000    |\n",
            "| value_loss         | 0.000115 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -13.4    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 37500    |\n",
            "| value_loss         | 0.000111 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.3     |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 38000    |\n",
            "| value_loss         | 3.43e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0728  |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 7700     |\n",
            "| policy_entropy     | 1.09     |\n",
            "| total_timesteps    | 38500    |\n",
            "| value_loss         | 0.000301 |\n",
            "---------------------------------\n",
            "571.6273290809751 512 438\n",
            "---------------------------------\n",
            "| explained_variance | 0.829    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 7800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 39000    |\n",
            "| value_loss         | 6.1e-08  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0505  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 7900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 39500    |\n",
            "| value_loss         | 0.000127 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -104     |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8000     |\n",
            "| policy_entropy     | 1.09     |\n",
            "| total_timesteps    | 40000    |\n",
            "| value_loss         | 0.000818 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.101    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 40500    |\n",
            "| value_loss         | 3.58e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0342  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 41000    |\n",
            "| value_loss         | 0.000245 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0209   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 41500    |\n",
            "| value_loss         | 0.000425 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00368  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 42000    |\n",
            "| value_loss         | 5.69e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.839    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 8500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 42500    |\n",
            "| value_loss         | 3.26e-06 |\n",
            "---------------------------------\n",
            "210.15596219416594 472 482\n",
            "---------------------------------\n",
            "| explained_variance | 0.402    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 8600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 43000    |\n",
            "| value_loss         | 3.91e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.69    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 43500    |\n",
            "| value_loss         | 0.000975 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.746   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 44000    |\n",
            "| value_loss         | 5.94e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00641  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 8900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 44500    |\n",
            "| value_loss         | 0.000833 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.184    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9000     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 45000    |\n",
            "| value_loss         | 1.21e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0904   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9100     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 45500    |\n",
            "| value_loss         | 1.42e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.847    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9200     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 46000    |\n",
            "| value_loss         | 3.18e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0112  |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9300     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 46500    |\n",
            "| value_loss         | 5.93e-06 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.754    |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 9400     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 47000    |\n",
            "| value_loss         | 2.27e-07 |\n",
            "---------------------------------\n",
            "256.5864882279818 492 439\n",
            "---------------------------------\n",
            "| explained_variance | 0.0115   |\n",
            "| fps                | 70       |\n",
            "| nupdates           | 9500     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 47500    |\n",
            "| value_loss         | 0.000354 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -144     |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9600     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 48000    |\n",
            "| value_loss         | 0.000171 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.551    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9700     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 48500    |\n",
            "| value_loss         | 2.7e-06  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.237    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9800     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 49000    |\n",
            "| value_loss         | 0.000437 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.99    |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 9900     |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 49500    |\n",
            "| value_loss         | 1.62e-05 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.195   |\n",
            "| fps                | 71       |\n",
            "| nupdates           | 10000    |\n",
            "| policy_entropy     | 1.1      |\n",
            "| total_timesteps    | 50000    |\n",
            "| value_loss         | 4.62e-07 |\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.a2c.a2c.A2C at 0x7ff17fc7dd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlmF3TQARG3a"
      },
      "source": [
        "model.save(\"A2C_FOREX\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}